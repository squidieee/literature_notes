# Duality of Depth Perception

## Visual Information in Surface and Depth Perception: Reconciling Pictures and Reality

Vishwanath 2010

A good chapter discusses how we see 3D in 2D pictures and how it is different from real 3D object. Author claims that the feeling of realism is associated with the existence of egocentric depth perception. The reason we see picture lack of true vivid depth is due to the absence of the egocentric depth perception. The duality of pictorial and real depth is in essence due to the distinction between relative and egocentrically scaled depth.

# Space Perception

Ware 2007

"To some extent, we have a choice between accurately judging the size of a depicted object as though it exists in a 3D
space and accurately judging its size on the picture plane (Hagen, 1974). The amount and effectiveness of the depth cues used will, to some extent, make it easy to see in one mode or the other."

# Dyadic Projected Spatial Augmented Reality

Benko 2014

A two-person projector-based immersive display that supports mono views and dyadic interactions. They have an interesting experiment that evaluates "object presence" presented by their display. They examined whether the participants can perceive projected virtual objects as spatial rather than appearing only at the projection surface (projected), and what factors affect their perception. They found while users are able to perform much better than chance on determining object size and distance in a controlled setting, subjects clearly performed worse at this task than if real objects were presented. Interacting directly with a virtual object lends a stronger sense of the position and size of the object (affordance) and watching the other user hold the object similarly helps (social cue).

# SIZE STUDY

# Towards Quantifying Depth and Size Perception in Virtual Environments

Rolland 1995

This paper examines important parameters within the computational model of stereo viewing. Though the model is for HMD, the analysis can still be quite useful to setup an experiment in assessing the depth/size perception. 

Then they conducted a study with relative depth perception. They found that virtual objects are perceived farther away than real objects. Moreover, no error in either calibration, image generation, or illumination could possibly account for the amplitude of the finding. Those

[Experimental Setup] The task is to judge the proximity in depth of a virtual object with respect to that of a physical object presented simultaneously. There are three conditions for the objects: both objects were real, virtual, or one was real and the other one was virtual. One of the objects was always fixed in location, and the other was moved in depth between trials. The screens displaying the virtual objects and the real objects were blanked for a few seconds between trials. Subjects performed a 2-alternative force choice task. They were asked to answer (via a key press) if the object on the right (the one translated in depth between trials), was closer or farther than the object on the left.

## Perceiving absolute scale in virtual environments: How theory and application have mutually informed the role of body-based perception

Stefanucci 2015

A Chapter of a book which surveys size perception studies.

The chapter first surveyed previous research on absolute distance study. The depth perception has also been mentioned as "egocentric distance", which measures the distance between the object and viewer, also called "absolute spatial information".

Most of previous study on the egocentric distance (depth), while there were few work on the "exocentric distance", which is the distance between two objects, neither of them being the viewer. This is "relative spatial information".

Different depth cues have different contributions on the absolute and relative spatial information. For example, binocular convergence, accommodation, linear perspective and familiar size are believed to contribute on the absolute spatial perception with limited range, usually within the personal space, which extends a bit beyond the distance that one can reach. 
Besides personal space, there are also action space and vista space. Action space extends from the far boundary of personal space to about 30m. Vista space is everything beyond action space. 

While there are extensive work on egocentric distance perception in VE, there is relative limited work on absolute size perception in VE, in comparison to real-word size estimates. 

The author categorizes size perception studies as 1. conscious awareness-based task; 2. action-based task; 3. affordance-based task;

- Awareness-based task: measure the conscious awareness that an observer has of his surrounding space. Eg:     rate the size/depth
- Action-based task: have people perform actions based on visual information about locations in the environment. Action-based measures of perceptual accuracy have been criticized because they also may be subject to biases and because it is difficult to disentangle inaccuracies in perception from inaccuracies in action.
- Affordance-based task: have people to make affordance judgments which relate environmental properties to an observerâ€™s actions. Eg: whether they can pass through an aperture hole with the body or wear a ring on their finger given different ring sizes. This is an evaluation methodology introduced by the author's group. They found it effective using grasping and reaching affordance judgement tasks.

The author's group worked on this topic and found perceived object appeared to be smaller in VE compared to real-world, which can be alleviated by increasing the complexity of context and adding stereo. They then further investigated the effectiveness of displaying virtual self-avatar and virtual hand to improve the size perception in VE. The research question is that *can body-based perception improved perceptual fidelity (on absolute size perception)*. They found the perceived size can be influenced by the shape and size of virtual hand.

## The effects of scene complexity, stereovision, and motion parallax on size constancy in a virtual environment

Luo2007

Subjects adjusted the size of a virtual bottle at different depth based on a real bottle. Independent variables are context-based depth cues (a table with size familiar to real object rendered in the scene), stereoscopsis and motion parallax. Only motion parallax is found to be non-significant to the user performance. 

## Size-Constancy in the CAVE

Kenyon2007

Size constancy is the phenomenon where an object is perceived as being the same size regardless of the distance from the observer, even if the size of retinal projection shrinks with increasing distance from the observer. Previous studies showed in *real environment* the performance suffers when the number of depth cues is reduced. Subjects adopt a size judgement approach based on the size of retinal projection. That is, instead of perceiving a constant size when changing the viewing distance, the object size appeared to shrink with increasing distance from the observer. Previous studies in the *virtual environment* show no size-constancy. But author thinks this is due to lack of depth cues as the previous work used sparse environment that eliminated surrounding objects which provide cues to the depth.

The paper tried to understand to what extent size-constancy could be experienced in this VE using CAVE. Subjects were tested using a rich environment with a number of surrounding items in the environment accompanied the virtual object, and a sparse environment where no surrounding items were provided and only the virtual object (a coke bottle) was visible. 

Subjects were instructed to adjust the size of bottle based on the size of a real bottle using a wand. They analyzed the ratio between the estimated size and actual size. They found size-constancy was more prevalent when the environment contained objects surrounding the test object in the scene to aid depth perception. Without these surrounding objects,most of the subjects adopted a visual angle (VA) performance when sizing the bottle.

## Evaluating the accuracy of size perception in real and virtual environments

Stefanucci2012

Affordance judgments measure perceived size by assessing what users believe they can or cannot do with objects in virtual environments. They used two affordance judgement tasks for grasping and reaching objects, in two viewing conditions (one real scene with a frame, the other with a fixed viewpoint perspective-corrected screen). For both judgment tasks,they found that users underestimated the size of the objects when judging their action capabilities for the desktop virtual environment in comparison to a real environment.

## Evaluating the accuracy of size perception on screen-based displays: Displayed objects appear smaller than real objects

Stefanucci2015

Same study as above but with stereo: stereo can reduce the effect of smaller real objects.

## Perceiving Layout and Knowing Distances: The integration, relative potency, and Contextual Use of Different Information about Depth

Cutting1995

todo

# DISTANCE STUDY

## Reaching measures of monocular distance perception : Forward versus side-to-side head movements and haptic feedback

Wickelgren2000

The study investigated which direction of *head movement* might best enable observers to perceive egocentric distance, especially when the information to be used to guide an action such as reaching. They compared performance of reaching by placing the tip of stylus on a target plate after having subjects performed forward and side-to-side head movements while observing the target. They found forward head movement condition has more accurate performance than the side-to-side condition.



## The influence of feedback on egocentric distance judgments in real and virtual environments

Mohler2006

They found within an HMD-VE that providing continuous visual feedback from the virtual world matched to the actorâ€™s walking speed led to less distance underestimation (more accurate judgments) in a posttest compared to pretest within the VE for both blind walking and verbal report measures (pretest-adaption session-posttest). 

## More than just perceptionâ€“action recalibration: Walking through a virtual environment causes rescaling of perceived space

Kelly2013

They replicated the general methodology of other studies allowing for active visual feedback while walking, but used two measures: blind walking and object size judgments. They found that both distance and size judgments improved linearly to come closer to the intended size/distance, consistent with the earlier work of Mohler et al. (2006) that used distance and verbal reports.

## Recalibration of Perceived Distance in Virtual Environments Occurs Rapidly and Transfers Asymmetrically Across Scale

Kelly2014

This work expands findings from Kelly2014 with the results that walking with continuous visual feedback to close objects (1-2m) led to distance estimates only to close objects, while walking to far objects led to improved estimates for both near and far objects. They also investigated how quickly improvement occurs as a result of walking interaction. They found that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement.

# Mitigating Incorrect Perception of Distance in Virtual Reality through Personalized Rendering Manipulation

Ponto 2019

This work seeks to reduce the effect of the depth-underestimation in VR by customized warping the distance. They first measured the depth error using a blink throwing task. They applied the warping using two models: both with a linear multiplier and one with an additional offset. Then they used three tasks: the same blind-throwing task, a size-matching task and a plank-orientation task to evaluate the warping in four viewing conditions: real, virtual-no warping, virtual-warping, and virtual-warping-offset. They found the virtual-warping-offset over-corrected the depth hence has the worst performance. The virtual-warping then has a better performance than no-warping. Still the real viewing condition is the best.

The idea behind the paper is useful and interesting: if there is systematic depth underestimation anyway, why not adjust rendering to correct it. However, using blind throwing task to measure the degree of warping does not seem to be reliable enough as throwing task itself introduces error caused by action of throwing. Furthermore, the warping only has linear model alternatives. It is not that clear why only linear models are chosen. From the result, the warping seems to be a non-linear function influenced by the distance, which is probably their future work that will address it.

# Orientation Study

## Rotating Virtual Objects with Real Handles

Colin Ware 1999

This paper conducts a series of experiments that tried to find out why rotating in virtual environment is far slower than it takes to rotate a real object. They find out: 

1. Placing the hand in the same spot as the virtual object appears will improve the task performance.
2. The result failed to support the idea that it is important for the shape of an object held in the hand to be the same as the shape of the object seen.
3. The results failed to show an advantage of two handed rotation vs. one handed input.
4. Whether the object is being rotated to a new, randomly determined orientation will greatly influence the task performance, compared to always rotating to the same position.

Good experiment setup: Look at these experiments setups in details when designing rotation/scaling task.

## Usability analysis of 3D rotation techniques

Hinckley 1997

 They examined the effect of input device shape on object rotation. They compared mouse and a 3 DoF Arcball input. Test users rotated the house model on the right side of the screen to match the orientation shown at the left. After each trial, performance was rated as "Excellent!" (shortest-arc rotation less than 5.7 degrees), "Good Match!" (less than 7.6 degrees), or "Not good enough, try harder next time." The main motivation for this rating system was to encourage subjects to achieve a high level of accuracy. 

They found that for the orientation matching task used in this experiment, users can take advantage of the integrated degrees of freedom provided by multidimensional input without necessarily sacrificing precision: using multidimensional input, users completed the experimental task up to 36% faster without any statistically detectable loss of accuracy. They also reported detailed observations of common usability problems of the multidimensional input technique. 

## Effects of head tracking and stereo on non-isomorphic 3D rotation

LaViola 2008
An experimental study that explores how head tracking and stereo affect user performance when rotating 3D virtual objects using isomorphic and non-isomorphic rotation techniques. Subjects performed the rotation task with significantly less error with head tracking/stereo and no head tracking/stereo than with no head tracking/no stereo, regardless of rotation technique. They used a solid shaded 3D model of a house from a randomly generated orientation into a target orientation. It is not quite clear how they indicated the target orientation with the stimulus.They measured the completion time when the orientation error was below a threshold, the house would immediately disappear and reappear in a new random orientation, indicating that the trial had been accomplished.

# Unclassified: todo

## Non-Isomorphic 3D Rotational Techniques 

Fels 1998

[todo]


## An exploration of non-isomorphic 3D rotation in surround screen virtual environments

LaViola

[todo]

# Docking Study

## Hybridspace: Integrating 3d freehand input and stereo viewing into traditional desktop applications

Grossman 2014

[todo]

## The GlobeFish and the GlobeMouse

[todo]


## Influence of degrees of freedom's manipulation on performances during orientation tasks in virtual reality environments

Veit 2009
[todo]...

## Quantifying coordination in multiple DOF movement and its application to evaluating 6 DOF input devices

Zhai 1998

A good paper that measures the coordination of multiple DoF movement when conducting various 3D tasks. 
[todo]